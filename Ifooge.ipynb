{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kainatquaderee/Ifooge/blob/main/Ifooge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep active"
      ],
      "metadata": {
        "id": "WTvdjKrsF9zV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep Colab notebook active\n",
        "from google.colab import output\n",
        "\n",
        "code = '''\n",
        "function KeepColabAlive() {\n",
        "    setInterval(() => {\n",
        "        console.log(\"Colab is running\");\n",
        "        document.querySelector(\"colab-toolbar-button#connect\").click();\n",
        "    }, 60000); // Change the interval if needed (60000 ms = 1 minute)\n",
        "}\n",
        "KeepColabAlive();\n",
        "'''\n",
        "\n",
        "output.eval_js(code)\n",
        "print(\"KeepAlive script is running\")"
      ],
      "metadata": {
        "id": "DjUGNX_sCKv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# version 1"
      ],
      "metadata": {
        "id": "4GGPKOvFEbIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjTHWta7BJqB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "43f66274-96d4-4bbe-cbbb-51d27af5e98f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-ea0c96222b63>, line 107)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ea0c96222b63>\"\u001b[0;36m, line \u001b[0;32m107\u001b[0m\n\u001b[0;31m    with\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers diffusers gradio\n",
        "!pip install git+https://github.com/TencentARC/GFPGAN.git\n",
        "!pip install basicsr\n",
        "!pip install git+https://github.com/xinntao/Real-ESRGAN.git\n",
        "\n",
        "# Fix the import issue in basicsr library\n",
        "!sed -i 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' /usr/local/lib/python3.*/dist-packages/basicsr/data/degradations.py\n",
        "\n",
        "# Keep Colab notebook active\n",
        "from google.colab import output\n",
        "\n",
        "code = '''\n",
        "function KeepColabAlive() {\n",
        "    setInterval(() => {\n",
        "        console.log(\"Colab is running\");\n",
        "        document.querySelector(\"colab-toolbar-button#connect\").click();\n",
        "    }, 60000); // Change the interval if needed (60000 ms = 1 minute)\n",
        "}\n",
        "KeepColabAlive();\n",
        "'''\n",
        "\n",
        "output.eval_js(code)\n",
        "print(\"KeepAlive script is running\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
        "import gradio as gr\n",
        "from gfpgan import GFPGANer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load GFPGAN for face restoration\n",
        "gfpgan = GFPGANer(\n",
        "    model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n",
        "    upscale=1,\n",
        "    arch='clean',\n",
        "    channel_multiplier=2,\n",
        "    bg_upsampler=None\n",
        ")\n",
        "\n",
        "# Function to fix faces\n",
        "def fix_faces_func(image):\n",
        "    image = np.array(image)\n",
        "    _, _, restored_image = gfpgan.enhance(image, has_aligned=False, only_center_face=False)\n",
        "    return Image.fromarray(restored_image)\n",
        "\n",
        "# Function to load model from Hugging Face\n",
        "def load_model(model_id):\n",
        "    global pipe, img2img_pipe\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "    pipe.to(\"cuda\")\n",
        "    pipe.safety_checker = dummy_checker\n",
        "    img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "    img2img_pipe.to(\"cuda\")\n",
        "    img2img_pipe.safety_checker = dummy_checker\n",
        "\n",
        "# Remove the safety checker\n",
        "def dummy_checker(images, **kwargs):\n",
        "    return images, [False] * len(images)\n",
        "\n",
        "# Function to generate images\n",
        "def generate_image(model_id, prompt=\"\", negative_prompt=\"lowres, bad quality, deformed, disfigured\", num_inference_steps=50, guidance_scale=7.5, fix_faces=False):\n",
        "    load_model(model_id)\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        # Generate initial image\n",
        "        image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale).images[0]\n",
        "\n",
        "    if fix_faces:\n",
        "        image = fix_faces_func(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Function to generate images from an input image\n",
        "def generate_img2img(model_id, init_image, prompt=\"\", negative_prompt=\"lowres, bad quality, deformed, disfigured\", num_inference_steps=50, guidance_scale=7.5, denoising_strength=0.75, fix_faces=False):\n",
        "    load_model(model_id)\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        # Generate image from input image\n",
        "        image = img2img_pipe(prompt, negative_prompt=negative_prompt, init_image=init_image, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, strength=denoising_strength).images[0]\n",
        "\n",
        "    if fix_faces:\n",
        "        image = fix_faces_func(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Custom CSS for a black modern theme\n",
        "css = \"\"\"\n",
        "body {background-color: #121212; color: #ffffff;}\n",
        ".gradio-container {background-color: #121212 !important; border-color: #333333 !important;}\n",
        ".gr-button {background-color: #333333 !important; color: #ffffff !important; border-color: #333333 !important;}\n",
        ".gr-button:hover {background-color: #444444 !important; border-color: #444444 !important;}\n",
        ".gr-textbox {background-color: #1f1f1f !important; color: #ffffff !important; border-color: #333333 !important;}\n",
        ".gr-slider {color: #ffffff !important;}\n",
        ".gr-checkbox label {color: #ffffff !important;}\n",
        "\"\"\"\n",
        "\n",
        "# Create a Gradio interface with two tabs\n",
        "iface = gr.Blocks(css=css)\n",
        "\n",
        "with iface:\n",
        "    gr.Markdown(\"# ifooge\\nGenerate images using Stable Diffusion and GFPGAN. By Kaiden Quinart.\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Text-to-Image\"):\n",
        "            gr.Markdown(\"## Text-to-Image\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_id = gr.Textbox(label=\"Hugging Face Model ID\", value=\"stablediffusionapi/realistic-vision-vae-5\")\n",
        "                    prompt = gr.Textbox(label=\"Prompt Details\")\n",
        "                    negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"lowres, bad quality, deformed, disfigured\")\n",
        "                    num_inference_steps = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=50, step=1)\n",
        "                    guidance_scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=20.0, value=7.5)\n",
        "                    fix_faces = gr.Checkbox(label=\"Fix Faces\")\n",
        "                with gr.Column():\n",
        "                    generate_button = gr.Button(\"Generate Image\")\n",
        "                    output_image = gr.Image(label=\"Generated Image\")\n",
        "\n",
        "            generate_button.click(generate_image, inputs=[model_id, prompt, negative_prompt, num_inference_steps, guidance_scale, fix_faces], outputs=output_image)\n",
        "\n",
        "        with gr.TabItem(\"Image-to-Image\"):\n",
        "            gr.Markdown(\"## Image-to-Image\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_id_img2img = gr.Textbox(label=\"Hugging Face Model ID\", value=\"stablediffusionapi/realistic-vision-vae-5\")\n",
        "                    init_image = gr.Image(label=\"Initial Image\")\n",
        "                    prompt_img2img = gr.Textbox(label=\"Prompt Details\")\n",
        "                    negative_prompt_img2img = gr.Textbox(label=\"Negative Prompt\", value=\"lowres, bad quality, deformed, disfigured\")\n",
        "                    num_inference_steps_img2img = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=50, step=1)\n",
        "                    guidance_scale_img2img = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=20.0, value=7.5)\n",
        "                    denoising_strength = gr.Slider(label=\"Denoising Strength\", minimum=0.0, maximum=1.0, value=0.75)\n",
        "                    fix_faces_img2img = gr.Checkbox(label=\"Fix Faces\")\n",
        "                with gr.Column():\n",
        "                    generate_button_img2img = gr.Button(\"Generate Image\")\n",
        "                    output_image_img2img = gr.Image(label=\"Generated Image\")\n",
        "\n",
        "            generate_button_img2img.click(generate_img2img, inputs=[model_id_img2img, init_image, prompt_img2img, negative_prompt_img2img, num_inference_steps_img2img, guidance_scale_img2img, denoising_strength, fix_faces_img2img], outputs=output_image_img2img)\n",
        "\n",
        "# Launch the interface with debug mode enabled\n",
        "iface.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# version 2"
      ],
      "metadata": {
        "id": "HrDtfJ_3ErK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers diffusers gradio\n",
        "!pip install git+https://github.com/TencentARC/GFPGAN.git\n",
        "!pip install basicsr\n",
        "!pip install git+https://github.com/xinntao/Real-ESRGAN.git\n",
        "\n",
        "# Fix the import issue in basicsr library\n",
        "!sed -i 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' /usr/local/lib/python3.*/dist-packages/basicsr/data/degradations.py\n",
        "\n",
        "# Keep Colab notebook active\n",
        "from google.colab import output\n",
        "\n",
        "code = '''\n",
        "function KeepColabAlive() {\n",
        "    setInterval(() => {\n",
        "        console.log(\"Colab is running\");\n",
        "        document.querySelector(\"colab-toolbar-button#connect\").click();\n",
        "    }, 60000); // Change the interval if needed (60000 ms = 1 minute)\n",
        "}\n",
        "KeepColabAlive();\n",
        "'''\n",
        "\n",
        "output.eval_js(code)\n",
        "print(\"KeepAlive script is running\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
        "import gradio as gr\n",
        "from gfpgan import GFPGANer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load GFPGAN for face restoration\n",
        "gfpgan = GFPGANer(\n",
        "    model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n",
        "    upscale=1,\n",
        "    arch='clean',\n",
        "    channel_multiplier=2,\n",
        "    bg_upsampler=None\n",
        ")\n",
        "\n",
        "# Function to fix faces\n",
        "def fix_faces_func(image):\n",
        "    image = np.array(image)\n",
        "    _, _, restored_image = gfpgan.enhance(image, has_aligned=False, only_center_face=False)\n",
        "    return Image.fromarray(restored_image)\n",
        "\n",
        "# Function to load model from Hugging Face\n",
        "def load_model(model_id):\n",
        "    global pipe, img2img_pipe\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "    pipe.to(\"cuda\")\n",
        "    pipe.safety_checker = dummy_checker\n",
        "    img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "    img2img_pipe.to(\"cuda\")\n",
        "    img2img_pipe.safety_checker = dummy_checker\n",
        "\n",
        "# Remove the safety checker\n",
        "def dummy_checker(images, **kwargs):\n",
        "    return images, [False] * len(images)\n",
        "\n",
        "# Function to generate images\n",
        "def generate_image(model_id, prompt=\"\", negative_prompt=\"lowres, bad quality, deformed, disfigured\", num_inference_steps=50, guidance_scale=7.5, fix_faces=False, seed=None, height=512, width=512):\n",
        "    load_model(model_id)\n",
        "    if seed is None:\n",
        "        seed = np.random.randint(0, 2**32 - 1)\n",
        "    generator = torch.manual_seed(seed)\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        # Generate initial image\n",
        "        image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, height=height, width=width, generator=generator).images[0]\n",
        "\n",
        "    if fix_faces:\n",
        "        image = fix_faces_func(image)\n",
        "\n",
        "    return image, seed\n",
        "\n",
        "# Function to generate images from an input image\n",
        "def generate_img2img(model_id, init_image, prompt=\"\", negative_prompt=\"lowres, bad quality, deformed, disfigured\", num_inference_steps=50, guidance_scale=7.5, denoising_strength=0.75, fix_faces=False, seed=None, height=512, width=512):\n",
        "    load_model(model_id)\n",
        "    if seed is None:\n",
        "        seed = np.random.randint(0, 2**32 - 1)\n",
        "    generator = torch.manual_seed(seed)\n",
        "    init_image = init_image.convert(\"RGB\")\n",
        "    init_image = init_image.resize((width, height))\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        # Generate image from input image\n",
        "        image = img2img_pipe(prompt, negative_prompt=negative_prompt, init_image=init_image, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, strength=denoising_strength, generator=generator).images[0]\n",
        "\n",
        "    if fix_faces:\n",
        "        image = fix_faces_func(image)\n",
        "\n",
        "    return image, seed\n",
        "\n",
        "# Custom CSS for a black modern theme\n",
        "css = \"\"\"\n",
        "body {background-color: #121212; color: #ffffff;}\n",
        ".gradio-container {background-color: #121212 !important; border-color: #333333 !important;}\n",
        ".gr-button {background-color: #333333 !important; color: #ffffff !important; border-color: #333333 !important;}\n",
        ".gr-button:hover {background-color: #444444 !important; border-color: #444444 !important;}\n",
        ".gr-textbox {background-color: #1f1f1f !important; color: #ffffff !important; border-color: #333333 !important;}\n",
        ".gr-slider {color: #ffffff !important;}\n",
        ".gr-checkbox label {color: #ffffff !important;}\n",
        "\"\"\"\n",
        "\n",
        "# Create a Gradio interface with two tabs\n",
        "iface = gr.Blocks(css=css)\n",
        "\n",
        "with iface:\n",
        "    gr.Markdown(\"# ifooge\\nGenerate images using Stable Diffusion and GFPGAN. By Kaiden Quinart.\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Text-to-Image\"):\n",
        "            gr.Markdown(\"## Text-to-Image\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_id = gr.Textbox(label=\"Hugging Face Model ID\", value=\"stablediffusionapi/realistic-vision-vae-5\")\n",
        "                    prompt = gr.Textbox(label=\"Prompt Details\")\n",
        "                    negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"lowres, bad quality, deformed, disfigured\")\n",
        "                    num_inference_steps = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=50, step=1)\n",
        "                    guidance_scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=20.0, value=7.5)\n",
        "                    fix_faces = gr.Checkbox(label=\"Fix Faces\")\n",
        "                    seed = gr.Number(label=\"Seed\", value=None, interactive=True)\n",
        "                    height = gr.Slider(label=\"Height\", minimum=256, maximum=1024, value=512, step=64)\n",
        "                    width = gr.Slider(label=\"Width\", minimum=256, maximum=1024, value=512, step=64)\n",
        "                with gr.Column():\n",
        "                    generate_button = gr.Button(\"Generate Image\")\n",
        "                    output_image = gr.Image(label=\"Generated Image\")\n",
        "                    output_seed = gr.Textbox(label=\"Seed Used\")\n",
        "\n",
        "            generate_button.click(generate_image, inputs=[model_id, prompt, negative_prompt, num_inference_steps, guidance_scale, fix_faces, seed, height, width], outputs=[output_image, output_seed])\n",
        "\n",
        "        with gr.TabItem(\"Image-to-Image\"):\n",
        "            gr.Markdown(\"## Image-to-Image\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_id_img2img = gr.Textbox(label=\"Hugging Face Model ID\", value=\"stablediffusionapi/realistic-vision-vae-5\")\n",
        "                    init_image = gr.Image(label=\"Initial Image\")\n",
        "                    prompt_img2img = gr.Textbox(label=\"Prompt Details\")\n",
        "                    negative_prompt_img2img = gr.Textbox(label=\"Negative Prompt\", value=\"lowres, bad quality, deformed, disfigured\")\n",
        "                    num_inference_steps_img2img = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=50, step=1)\n",
        "                    guidance_scale_img2img = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=20.0, value=7.5)\n",
        "                    denoising_strength = gr.Slider(label=\"Denoising Strength\", minimum=0.0, maximum=1.0, value=0.75)\n",
        "                    fix_faces_img2img = gr.Checkbox(label=\"Fix Faces\")\n",
        "                    seed_img2img = gr.Number(label=\"Seed\", value=None, interactive=True)\n",
        "                    height_img2img = gr.Slider(label=\"Height\", minimum=256, maximum=1024, value=512, step=64)\n",
        "                    width_img2img = gr.Slider(label=\"Width\", minimum=256, maximum=1024, value=512, step=64)\n",
        "                with gr.Column():\n",
        "                    generate_button_img2img = gr.Button(\"Generate Image\")\n",
        "                    output_image_img2img = gr.Image(label=\"Generated Image\")\n",
        "                    output_seed_img2img = gr.Textbox(label=\"Seed Used\")\n",
        "\n",
        "            generate_button_img2img.click(generate_img2img, inputs=[model_id_img2img, init_image, prompt_img2img, negative_prompt_img2img, num_inference_steps_img2img, guidance_scale_img2img, denoising_strength, fix_faces_img2img, seed_img2img, height_img2img, width_img2img], outputs=[output_image_img2img, output_seed_img2img])\n",
        "\n",
        "# Launch the interface with debug mode enabled\n",
        "iface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "9CxGqGYuEv4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# animatediff"
      ],
      "metadata": {
        "id": "D0aPC2rNi44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers diffusers gradio\n",
        "!pip install git+https://github.com/TencentARC/GFPGAN.git\n",
        "!pip install basicsr\n",
        "!pip install git+https://github.com/xinntao/Real-ESRGAN.git\n",
        "\n",
        "# Fix the import issue in basicsr library\n",
        "!sed -i 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' /usr/local/lib/python3.*/dist-packages/basicsr/data/degradations.py\n",
        "\n",
        "# Keep Colab notebook active\n",
        "from google.colab import output\n",
        "\n",
        "code = '''\n",
        "function KeepColabAlive() {\n",
        "    setInterval(() => {\n",
        "        console.log(\"Colab is running\");\n",
        "        document.querySelector(\"colab-toolbar-button#connect\").click();\n",
        "    }, 60000); // Change the interval if needed (60000 ms = 1 minute)\n",
        "}\n",
        "KeepColabAlive();\n",
        "'''\n",
        "\n",
        "output.eval_js(code)\n",
        "print(\"KeepAlive script is running\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, AnimateDiffPipeline, DDIMScheduler, MotionAdapter\n",
        "import gradio as gr\n",
        "from gfpgan import GFPGANer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from diffusers.utils import export_to_gif\n",
        "\n",
        "# Load GFPGAN for face restoration\n",
        "gfpgan = GFPGANer(\n",
        "    model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n",
        "    upscale=1,\n",
        "    arch='clean',\n",
        "    channel_multiplier=2,\n",
        "    bg_upsampler=None\n",
        ")\n",
        "\n",
        "# Function to fix faces\n",
        "def fix_faces_func(image):\n",
        "    image = np.array(image)\n",
        "    _, _, restored_image = gfpgan.enhance(image, has_aligned=False, only_center_face=False)\n",
        "    return Image.fromarray(restored_image)\n",
        "\n",
        "# Function to load model from Hugging Face\n",
        "def load_model(model_id):\n",
        "    global pipe, img2img_pipe\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "    pipe.to(\"cuda\")\n",
        "    pipe.safety_checker = dummy_checker\n",
        "    img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "    img2img_pipe.to(\"cuda\")\n",
        "    img2img_pipe.safety_checker = dummy_checker\n",
        "\n",
        "# Remove the safety checker\n",
        "def dummy_checker(images, **kwargs):\n",
        "    return images, [False] * len(images)\n",
        "\n",
        "# Function to generate images\n",
        "def generate_image(model_id, prompt=\"\", negative_prompt=\"lowres, bad quality, deformed, disfigured\", num_inference_steps=50, guidance_scale=7.5, fix_faces=False, seed=None, height=512, width=512):\n",
        "    load_model(model_id)\n",
        "    if seed is None:\n",
        "        seed = np.random.randint(0, 2**32 - 1)\n",
        "    generator = torch.manual_seed(seed)\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        # Generate initial image\n",
        "        image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, height=height, width=width, generator=generator).images[0]\n",
        "\n",
        "    if fix_faces:\n",
        "        image = fix_faces_func(image)\n",
        "\n",
        "    return image, seed\n",
        "\n",
        "# Function to generate images from an input image\n",
        "def generate_img2img(model_id, init_image, prompt=\"\", negative_prompt=\"lowres, bad quality, deformed, disfigured\", num_inference_steps=50, guidance_scale=7.5, denoising_strength=0.75, fix_faces=False, seed=None, height=512, width=512):\n",
        "    load_model(model_id)\n",
        "    if seed is None:\n",
        "        seed = np.random.randint(0, 2**32 - 1)\n",
        "    generator = torch.manual_seed(seed)\n",
        "    init_image = init_image.convert(\"RGB\")\n",
        "    init_image = init_image.resize((width, height))\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        # Generate image from input image\n",
        "        image = img2img_pipe(prompt, negative_prompt=negative_prompt, init_image=init_image, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, strength=denoising_strength, generator=generator).images[0]\n",
        "\n",
        "    if fix_faces:\n",
        "        image = fix_faces_func(image)\n",
        "\n",
        "    return image, seed\n",
        "\n",
        "# Function to generate text-to-video\n",
        "def generate_text2video(prompt, negative_prompt=\"bad quality, worse quality\", num_frames=16, guidance_scale=7.5, num_inference_steps=25, seed=None):\n",
        "    if seed is None:\n",
        "        seed = np.random.randint(0, 2**32 - 1)\n",
        "    generator = torch.manual_seed(seed)\n",
        "\n",
        "    adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n",
        "    model_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\n",
        "    pipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\n",
        "    scheduler = DDIMScheduler.from_pretrained(\n",
        "        model_id,\n",
        "        subfolder=\"scheduler\",\n",
        "        clip_sample=False,\n",
        "        timestep_spacing=\"linspace\",\n",
        "        beta_schedule=\"linear\",\n",
        "        steps_offset=1,\n",
        "    )\n",
        "    pipe.scheduler = scheduler\n",
        "\n",
        "    pipe.enable_vae_slicing()\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    output = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_frames=num_frames,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        generator=generator,\n",
        "    )\n",
        "\n",
        "    frames = output.frames[0]\n",
        "    gif_path = \"animation.gif\"\n",
        "    export_to_gif(frames, gif_path)\n",
        "\n",
        "    return gif_path, seed\n",
        "\n",
        "# Custom CSS for a black modern theme\n",
        "css = \"\"\"\n",
        "body {background-color: #121212; color: #ffffff;}\n",
        ".gradio-container {background-color: #121212 !important; border-color: #333333 !important;}\n",
        ".gr-button {background-color: #333333 !important; color: #ffffff !important; border-color: #333333 !important;}\n",
        ".gr-button:hover {background-color: #444444 !important; border-color: #444444 !important;}\n",
        ".gr-textbox {background-color: #1f1f1f !important; color: #ffffff !important; border-color: #333333 !important;}\n",
        ".gr-slider {color: #ffffff !important;}\n",
        ".gr-checkbox label {color: #ffffff !important;}\n",
        "\"\"\"\n",
        "\n",
        "# Create a Gradio interface with three tabs\n",
        "iface = gr.Blocks(css=css)\n",
        "\n",
        "with iface:\n",
        "    gr.Markdown(\"# ifooge\\nGenerate images and videos using Stable Diffusion, GFPGAN, and AnimateDiff. By Kaiden Quinart.\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Text-to-Image\"):\n",
        "            gr.Markdown(\"## Text-to-Image\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_id = gr.Textbox(label=\"Hugging Face Model ID\", value=\"stablediffusionapi/realistic-vision-vae-5\")\n",
        "                    prompt = gr.Textbox(label=\"Prompt Details\")\n",
        "                    negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"lowres, bad quality, deformed, disfigured\")\n",
        "                    num_inference_steps = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=50, step=1)\n",
        "                    guidance_scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=20.0, value=7.5)\n",
        "                    fix_faces = gr.Checkbox(label=\"Fix Faces\")\n",
        "                    seed = gr.Number(label=\"Seed\", value=None, interactive=True)\n",
        "                    height = gr.Slider(label=\"Height\", minimum=256, maximum=1024, value=512, step=64)\n",
        "                    width = gr.Slider(label=\"Width\", minimum=256, maximum=1024, value=512, step=64)\n",
        "                with gr.Column():\n",
        "                    generate_button = gr.Button(\"Generate Image\")\n",
        "                    output_image = gr.Image(label=\"Generated Image\")\n",
        "                    output_seed = gr.Textbox(label=\"Seed Used\")\n",
        "\n",
        "            generate_button.click(generate_image, inputs=[model_id, prompt, negative_prompt, num_inference_steps, guidance_scale, fix_faces, seed, height, width], outputs=[output_image, output_seed])\n",
        "\n",
        "        with gr.TabItem(\"Image-to-Image\"):\n",
        "            gr.Markdown(\"## Image-to-Image\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    model_id_img2img = gr.Textbox(label=\"Hugging Face Model ID\", value=\"stablediffusionapi/realistic-vision-vae-5\")\n",
        "                    init_image = gr.Image(label=\"Initial Image\")\n",
        "                    prompt_img2img = gr.Textbox(label=\"Prompt Details\")\n",
        "                    negative_prompt_img2img = gr.Textbox(label=\"Negative Prompt\", value=\"lowres, bad quality, deformed, disfigured\")\n",
        "                    num_inference_steps_img2img = gr.Slider(label=\"Inference Steps\", minimum=1, maximum=100, value=50, step=1)\n",
        "                    guidance_scale_img2img = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=20.0, value=7.5)\n",
        "                    denoising_strength = gr.Slider(label=\"Denoising Strength\", minimum=0.0, maximum=1.0, value=0.75)\n",
        "                    fix_faces_img2img = gr.Checkbox(label=\"Fix Faces\")\n",
        "                    seed_img2img"
      ],
      "metadata": {
        "id": "_3YmexD9iufq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}